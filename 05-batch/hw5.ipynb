{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Spark version (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('hw5') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/windowstest/spark/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py\n",
      "Spark version is : 3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__file__)\n",
    "print(f\"Spark version is : {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 87MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-06 20:51:38--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240306T205138Z&X-Amz-Expires=300&X-Amz-Signature=2353eef31c9becdd6e7cdcf4d8c80a10bf3cb4c52bbf26e5151c1e44d08dd301&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-06 20:51:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240306T205138Z&X-Amz-Expires=300&X-Amz-Signature=2353eef31c9becdd6e7cdcf4d8c80a10bf3cb4c52bbf26e5151c1e44d08dd301&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz.1’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  83.0MB/s    in 0.2s    \n",
      "\n",
      "2024-03-06 20:51:39 (83.0 MB/s) - ‘fhv_tripdata_2019-10.csv.gz.1’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install fhv data\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  after installing this i you need to unzip the file like this\n",
    "\n",
    "`gzip -d fhv_tripdata_2019-10.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num,pickup_datetime,dropOff_datetime,PUlocationID,DOlocationID,SR_Flag,Affiliated_base_number\r\n",
      "B00009,2019-10-01 00:23:00,2019-10-01 00:35:00,264,264,,B00009\r\n",
      "B00013,2019-10-01 00:11:29,2019-10-01 00:13:22,264,264,,B00013\r\n",
      "B00014,2019-10-01 00:11:43,2019-10-01 00:37:20,264,264,,B00014\r\n",
      "B00014,2019-10-01 00:56:29,2019-10-01 00:57:47,264,264,,B00014\r\n",
      "B00014,2019-10-01 00:23:09,2019-10-01 00:28:27,264,264,,B00014\r\n",
      "B00021         ,2019-10-01 00:00:48,2019-10-01 00:07:12,129,129,,B00021         \r\n",
      "B00021         ,2019-10-01 00:47:23,2019-10-01 00:53:25,57,57,,B00021         \r\n",
      "B00021         ,2019-10-01 00:10:06,2019-10-01 00:19:50,173,173,,B00021         \r\n",
      "B00021         ,2019-10-01 00:51:37,2019-10-01 01:06:14,226,226,,B00021         \r\n"
     ]
    }
   ],
   "source": [
    "!head fhv_tripdata_2019-10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897493 entries, 0 to 1897492\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   dispatching_base_num    object \n",
      " 1   pickup_datetime         object \n",
      " 2   dropOff_datetime        object \n",
      " 3   PUlocationID            float64\n",
      " 4   DOlocationID            float64\n",
      " 5   SR_Flag                 float64\n",
      " 6   Affiliated_base_number  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 101.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2 = pd.read_csv('fhv_tripdata_2019-10.csv')\n",
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv with schema\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhv/2019/10/', mode ='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. FHV October 2019 partition size (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check with `ls -lh` \n",
    "- avg mean is 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "**Count records** \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 108,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 62,610\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Be aware of columns order when defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02784|2019-10-01 09:55:38|2019-10-01 10:05:43|          89|          85|   null|                  null|\n",
      "|              B01315|2019-10-05 15:13:04|2019-10-05 15:19:48|         264|          74|   null|                B01315|\n",
      "|              B01984|2019-10-12 17:13:00|2019-10-12 17:40:00|         264|          75|   null|                B01984|\n",
      "|              B00310|2019-10-15 10:55:04|2019-10-15 11:00:45|         264|         247|   null|                B03047|\n",
      "|              B00932|2019-10-08 06:58:42|2019-10-08 07:11:11|         264|          37|   null|                B00932|\n",
      "|              B01029|2019-10-10 14:45:00|2019-10-10 15:47:00|         264|         264|   null|                B01029|\n",
      "|              B01087|2019-10-14 18:41:24|2019-10-14 19:02:06|         261|         186|   null|                B01087|\n",
      "|              B03080|2019-10-05 14:49:10|2019-10-05 15:02:14|         264|          25|   null|                B02889|\n",
      "|              B03160|2019-10-10 12:50:00|2019-10-10 13:34:00|          77|          77|   null|                B02882|\n",
      "|              B02472|2019-10-16 14:12:36|2019-10-16 14:35:00|         264|         157|   null|                B02472|\n",
      "|              B01051|2019-10-05 22:06:46|2019-10-05 22:16:57|         264|         182|   null|                B01051|\n",
      "|              B02111|2019-10-08 14:58:52|2019-10-08 15:40:41|          98|          79|   null|                B02111|\n",
      "|              B00254|2019-10-03 20:33:11|2019-10-03 21:52:16|         246|         265|   null|                B02356|\n",
      "|              B00756|2019-10-16 10:58:00|2019-10-16 11:18:00|         264|         264|   null|                B00756|\n",
      "|              B02249|2019-10-04 19:55:49|2019-10-04 20:08:25|         264|         192|   null|                B02249|\n",
      "|              B03202|2019-10-13 07:39:33|2019-10-13 08:18:31|         264|         132|   null|                B03202|\n",
      "|              B00419|2019-10-11 08:33:12|2019-10-11 08:46:22|         182|         185|   null|                B00419|\n",
      "|              B02095|2019-10-09 11:16:00|2019-10-09 11:44:00|         264|         264|   null|                B02095|\n",
      "|              B02930|2019-10-05 22:06:15|2019-10-05 22:25:31|         264|          69|   null|                B02930|\n",
      "|              B01239|2019-10-07 20:08:15|2019-10-07 20:16:06|         264|          51|   null|                B02847|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropOff_datetime', 'timestamp'),\n",
       " ('PUlocationID', 'int'),\n",
       " ('DOlocationID', 'int'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('Affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|PUlocationID|Affiliated_base_number|\n",
      "+------------+----------------------+\n",
      "|          89|                  null|\n",
      "|         264|                B01315|\n",
      "|         264|                B01984|\n",
      "|         264|                B03047|\n",
      "|         264|                B00932|\n",
      "|         264|                B01029|\n",
      "|         261|                B01087|\n",
      "|         264|                B02889|\n",
      "|          77|                B02882|\n",
      "|         264|                B02472|\n",
      "+------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('PUlocationID','Affiliated_base_number').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15 trip count is : 62610\n",
      "+-----------+\n",
      "|pickup_date|\n",
      "+-----------+\n",
      "| 2019-10-15|\n",
      "| 2019-10-15|\n",
      "| 2019-10-15|\n",
      "| 2019-10-15|\n",
      "| 2019-10-15|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many taxi trips were there on the 15th of October?\n",
    "#Consider only trips that started on the 15th of October.\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "df= df \\\n",
    ".withColumn(\"pickup_date\", F.to_date(df.pickup_datetime)) \\\n",
    "\n",
    "trip_count = df.filter(df.pickup_date == \"2019-10-15\").select('pickup_date').count()\n",
    "\n",
    "print(f\"2019-10-15 trip count is : {trip_count}\")\n",
    "\n",
    "df.filter(df.pickup_date == \"2019-10-15\").select('pickup_date').show(5)\n",
    "# df.filter(df.pickup_datetime.cast(\"date\") == \"2019-10-15\").count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|pickup_date|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+\n",
      "|              B02784|2019-10-01 09:55:38|2019-10-01 10:05:43|          89|          85|   null|                  null| 2019-10-01|\n",
      "|              B01315|2019-10-05 15:13:04|2019-10-05 15:19:48|         264|          74|   null|                B01315| 2019-10-05|\n",
      "|              B01984|2019-10-12 17:13:00|2019-10-12 17:40:00|         264|          75|   null|                B01984| 2019-10-12|\n",
      "|              B00310|2019-10-15 10:55:04|2019-10-15 11:00:45|         264|         247|   null|                B03047| 2019-10-15|\n",
      "|              B00932|2019-10-08 06:58:42|2019-10-08 07:11:11|         264|          37|   null|                B00932| 2019-10-08|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windowstest/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhv_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql solution\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fhv_2019_10\n",
    "WHERE DATE(pickup_datetime) = '2019-10-15';\n",
    "\"\"\").show()\n",
    "\n",
    "# or \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM fhv_2019_10\n",
    "WHERE pickup_date = '2019-10-15';\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 631,152.50 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|max(trip_duration_hours)|\n",
      "+------------------------+\n",
      "|                631152.5|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df.withColumn(\"trip_duration_hours\", (col(\"dropoff_datetime\").cast(\"long\") - col(\"pickup_datetime\").cast(\"long\")) / 3600) \\\n",
    ".select(max('trip_duration_hours')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "| trip_date|longest_trip_hours|\n",
      "+----------+------------------+\n",
      "|2019-10-28|          631152.5|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql solution\n",
    "df.registerTempTable('fhv_2019_10')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(Pickup_datetime) AS trip_date,\n",
    "    MAX((UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / 3600) AS longest_trip_hours\n",
    "FROM\n",
    "    fhv_2019_10\n",
    "GROUP BY\n",
    "    trip_date\n",
    "ORDER BY\n",
    "    longest_trip_hours DESC\n",
    "LIMIT 1;\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>\n",
    "\n",
    "- East Chelsea\n",
    "- Jamaica Bay\n",
    "- Union Sq\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-06 20:52:26--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240306T205226Z&X-Amz-Expires=300&X-Amz-Signature=d9f342c96eecdbd99b55c5b7b8b05006eaa4a16fb8762ea778308824c558592b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-06 20:52:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240306%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240306T205226Z&X-Amz-Expires=300&X-Amz-Signature=d9f342c96eecdbd99b55c5b7b8b05006eaa4a16fb8762ea778308824c558592b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi_zone_lookup.csv.1’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.03K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2024-03-06 20:52:27 (2.63 MB/s) - ‘taxi_zone_lookup.csv.1’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install zone data\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_zone = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 265 entries, 0 to 264\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   LocationID    265 non-null    int64 \n",
      " 1   Borough       265 non-null    object\n",
      " 2   Zone          264 non-null    object\n",
      " 3   service_zone  263 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_temp_zone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_zone = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True),\n",
    "])\n",
    "\n",
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_zone) \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with zone lookup data\n",
    "joined_df = df.join(df_zone, df.PUlocationID == df_zone.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.groupBy(\"Zone\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent pickup location: Jamaica Bay\n"
     ]
    }
   ],
   "source": [
    "# Find the least frequent pickup location\n",
    "least_frequent_location = joined_df.groupBy(\"Zone\").count().orderBy(\"count\").select(\"Zone\").first()\n",
    "\n",
    "# Print the result\n",
    "print(\"Least frequent pickup location: {}\".format(least_frequent_location[\"Zone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
